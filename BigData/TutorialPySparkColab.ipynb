{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "h-buZtUBtnOp",
        "mr5GDyV93GOi",
        "FlPM5Da224mk",
        "ikvKjBuY6e7-",
        "7_zMjiAbClef",
        "AAlB-sLiUZGS"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tutorial 1: Configuración y carga de datos en PySpark"
      ],
      "metadata": {
        "id": "kbRRaZlBt6Nh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Paso 1: Configuración del entorno de PySpark en Colab"
      ],
      "metadata": {
        "id": "K6Iv32T_kgRf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKfMHXDwiZqF",
        "outputId": "22a1ed82-87c6-44eb-aaf8-6142a056f914"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Hit:2 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Ign:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:7 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:11 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:12 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "20 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.7/dist-packages (3.3.0)\n",
            "Requirement already satisfied: py4j==0.10.9.5 in /usr/local/lib/python3.7/dist-packages (from pyspark) (0.10.9.5)\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "#Bibliotecas para poder trabajar con Spark\n",
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.2.2/spark-3.2.2-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.2-bin-hadoop3.2.tgz  \n",
        "#Configuración de Spark con Python\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "\n",
        "#Estableciendo variable de entorno\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.2-bin-hadoop3.2\"\n",
        "\n",
        "#Buscando e inicializando la instalación de Spark\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")  \n",
        "%cd \"/content\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Paso 2: Selección y vista de los datos"
      ],
      "metadata": {
        "id": "3-DE0eskkyRA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv('sample_data/california_housing_test.csv')"
      ],
      "metadata": {
        "id": "ywqxFXoXJiv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "jH2hpm6PJYUK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "a3014dbb-09b0-4010-e832-294212fa108a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
              "0       -122.05     37.37                27.0       3885.0           661.0   \n",
              "1       -118.30     34.26                43.0       1510.0           310.0   \n",
              "2       -117.81     33.78                27.0       3589.0           507.0   \n",
              "3       -118.36     33.82                28.0         67.0            15.0   \n",
              "4       -119.67     36.33                19.0       1241.0           244.0   \n",
              "...         ...       ...                 ...          ...             ...   \n",
              "2995    -119.86     34.42                23.0       1450.0           642.0   \n",
              "2996    -118.14     34.06                27.0       5257.0          1082.0   \n",
              "2997    -119.70     36.30                10.0        956.0           201.0   \n",
              "2998    -117.12     34.10                40.0         96.0            14.0   \n",
              "2999    -119.63     34.42                42.0       1765.0           263.0   \n",
              "\n",
              "      population  households  median_income  median_house_value  \n",
              "0         1537.0       606.0         6.6085            344700.0  \n",
              "1          809.0       277.0         3.5990            176500.0  \n",
              "2         1484.0       495.0         5.7934            270500.0  \n",
              "3           49.0        11.0         6.1359            330000.0  \n",
              "4          850.0       237.0         2.9375             81700.0  \n",
              "...          ...         ...            ...                 ...  \n",
              "2995      1258.0       607.0         1.1790            225000.0  \n",
              "2996      3496.0      1036.0         3.3906            237200.0  \n",
              "2997       693.0       220.0         2.2895             62000.0  \n",
              "2998        46.0        14.0         3.2708            162500.0  \n",
              "2999       753.0       260.0         8.5608            500001.0  \n",
              "\n",
              "[3000 rows x 9 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-50c5cd55-12d7-4899-8556-698d8cb81cde\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>longitude</th>\n",
              "      <th>latitude</th>\n",
              "      <th>housing_median_age</th>\n",
              "      <th>total_rooms</th>\n",
              "      <th>total_bedrooms</th>\n",
              "      <th>population</th>\n",
              "      <th>households</th>\n",
              "      <th>median_income</th>\n",
              "      <th>median_house_value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-122.05</td>\n",
              "      <td>37.37</td>\n",
              "      <td>27.0</td>\n",
              "      <td>3885.0</td>\n",
              "      <td>661.0</td>\n",
              "      <td>1537.0</td>\n",
              "      <td>606.0</td>\n",
              "      <td>6.6085</td>\n",
              "      <td>344700.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-118.30</td>\n",
              "      <td>34.26</td>\n",
              "      <td>43.0</td>\n",
              "      <td>1510.0</td>\n",
              "      <td>310.0</td>\n",
              "      <td>809.0</td>\n",
              "      <td>277.0</td>\n",
              "      <td>3.5990</td>\n",
              "      <td>176500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-117.81</td>\n",
              "      <td>33.78</td>\n",
              "      <td>27.0</td>\n",
              "      <td>3589.0</td>\n",
              "      <td>507.0</td>\n",
              "      <td>1484.0</td>\n",
              "      <td>495.0</td>\n",
              "      <td>5.7934</td>\n",
              "      <td>270500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-118.36</td>\n",
              "      <td>33.82</td>\n",
              "      <td>28.0</td>\n",
              "      <td>67.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>49.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>6.1359</td>\n",
              "      <td>330000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-119.67</td>\n",
              "      <td>36.33</td>\n",
              "      <td>19.0</td>\n",
              "      <td>1241.0</td>\n",
              "      <td>244.0</td>\n",
              "      <td>850.0</td>\n",
              "      <td>237.0</td>\n",
              "      <td>2.9375</td>\n",
              "      <td>81700.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2995</th>\n",
              "      <td>-119.86</td>\n",
              "      <td>34.42</td>\n",
              "      <td>23.0</td>\n",
              "      <td>1450.0</td>\n",
              "      <td>642.0</td>\n",
              "      <td>1258.0</td>\n",
              "      <td>607.0</td>\n",
              "      <td>1.1790</td>\n",
              "      <td>225000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2996</th>\n",
              "      <td>-118.14</td>\n",
              "      <td>34.06</td>\n",
              "      <td>27.0</td>\n",
              "      <td>5257.0</td>\n",
              "      <td>1082.0</td>\n",
              "      <td>3496.0</td>\n",
              "      <td>1036.0</td>\n",
              "      <td>3.3906</td>\n",
              "      <td>237200.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2997</th>\n",
              "      <td>-119.70</td>\n",
              "      <td>36.30</td>\n",
              "      <td>10.0</td>\n",
              "      <td>956.0</td>\n",
              "      <td>201.0</td>\n",
              "      <td>693.0</td>\n",
              "      <td>220.0</td>\n",
              "      <td>2.2895</td>\n",
              "      <td>62000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2998</th>\n",
              "      <td>-117.12</td>\n",
              "      <td>34.10</td>\n",
              "      <td>40.0</td>\n",
              "      <td>96.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>46.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>3.2708</td>\n",
              "      <td>162500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2999</th>\n",
              "      <td>-119.63</td>\n",
              "      <td>34.42</td>\n",
              "      <td>42.0</td>\n",
              "      <td>1765.0</td>\n",
              "      <td>263.0</td>\n",
              "      <td>753.0</td>\n",
              "      <td>260.0</td>\n",
              "      <td>8.5608</td>\n",
              "      <td>500001.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3000 rows × 9 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-50c5cd55-12d7-4899-8556-698d8cb81cde')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-50c5cd55-12d7-4899-8556-698d8cb81cde button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-50c5cd55-12d7-4899-8556-698d8cb81cde');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paso 3: Crear la sesión de trabajo de Spark\n",
        "\n",
        "Ya seleccionado y visto el conjunto de datos comencemos a trabajar con PySpark. Para comenzar a trabajar con PySpark, debemos iniciar la sesión de Spark. Para esto realizaremos lo siguiente:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "1.   Importar SparkSession \n",
        "2.   Crear la sesión \n",
        "\n"
      ],
      "metadata": {
        "id": "Px0CvEobk43I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Verificar la funcionalidad de Pyspark \n",
        "from pyspark.sql import SparkSession\n",
        "spark_session = SparkSession.builder.appName('PySpark_prueba1').getOrCreate()\n",
        "spark_session"
      ],
      "metadata": {
        "id": "hqEN5b1eKRTn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "6658d749-adc6-489f-fe53-3d8210e80cfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7ffaa1f26050>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://9058f9c207bc:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.2.2</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>PySpark_prueba1</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La SparkSession contiene los siguiente elementos:\n",
        "\n",
        "\n",
        "*   Version: La versión de Spark\n",
        "*   Master: Como estamos trabajando en un sistema en la nube pero no distribuido nos devuelve local, sin embargo, si tuvieramos un sistema distribuido aquí entonces podríamos tener diferentes clústeres, así como primero habrá un maestro y luego una estructura similar a un árbol (cluster_1, cluster_2 ... cluster_n).\n",
        "*   AppName: Nombre de la aplicación."
      ],
      "metadata": {
        "id": "mbRahpVtmPBu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Paso 4: Cargar los datos para manipularlos dentro de Spark"
      ],
      "metadata": {
        "id": "iADBYL1_tvfU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_spark = spark_session.read.csv('sample_data/california_housing_train.csv')\n",
        "df_spark"
      ],
      "metadata": {
        "id": "qJdqDvH1Ke0F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "978c10c8-4cca-4b79-810d-f0c4c72069d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string, _c6: string, _c7: string, _c8: string]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En el caso de PySpark para visualizar los datos tenemos la función *show()* ques similar a *head()* de pandas con algunas diferencias como:\n",
        "\n",
        "\n",
        "1.   Mostrar 20 registro en lugar de 5\n",
        "2.   La apariencia de los datos\n",
        "3.   En lugar de tomar la primera fila como encabezados la incluye como un registro y coloca _c1 a _cn como nombre de la columna.\n",
        "\n"
      ],
      "metadata": {
        "id": "OkNMWmBnm6QO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_spark.show()"
      ],
      "metadata": {
        "id": "zT2_YaGFnynL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ce301c9-e87e-4892-b03e-736bcb75e496"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+---------+------------------+-----------+--------------+-----------+-----------+-------------+------------------+\n",
            "|        _c0|      _c1|               _c2|        _c3|           _c4|        _c5|        _c6|          _c7|               _c8|\n",
            "+-----------+---------+------------------+-----------+--------------+-----------+-----------+-------------+------------------+\n",
            "|  longitude| latitude|housing_median_age|total_rooms|total_bedrooms| population| households|median_income|median_house_value|\n",
            "|-114.310000|34.190000|         15.000000|5612.000000|   1283.000000|1015.000000| 472.000000|     1.493600|      66900.000000|\n",
            "|-114.470000|34.400000|         19.000000|7650.000000|   1901.000000|1129.000000| 463.000000|     1.820000|      80100.000000|\n",
            "|-114.560000|33.690000|         17.000000| 720.000000|    174.000000| 333.000000| 117.000000|     1.650900|      85700.000000|\n",
            "|-114.570000|33.640000|         14.000000|1501.000000|    337.000000| 515.000000| 226.000000|     3.191700|      73400.000000|\n",
            "|-114.570000|33.570000|         20.000000|1454.000000|    326.000000| 624.000000| 262.000000|     1.925000|      65500.000000|\n",
            "|-114.580000|33.630000|         29.000000|1387.000000|    236.000000| 671.000000| 239.000000|     3.343800|      74000.000000|\n",
            "|-114.580000|33.610000|         25.000000|2907.000000|    680.000000|1841.000000| 633.000000|     2.676800|      82400.000000|\n",
            "|-114.590000|34.830000|         41.000000| 812.000000|    168.000000| 375.000000| 158.000000|     1.708300|      48500.000000|\n",
            "|-114.590000|33.610000|         34.000000|4789.000000|   1175.000000|3134.000000|1056.000000|     2.178200|      58400.000000|\n",
            "|-114.600000|34.830000|         46.000000|1497.000000|    309.000000| 787.000000| 271.000000|     2.190800|      48100.000000|\n",
            "|-114.600000|33.620000|         16.000000|3741.000000|    801.000000|2434.000000| 824.000000|     2.679700|      86500.000000|\n",
            "|-114.600000|33.600000|         21.000000|1988.000000|    483.000000|1182.000000| 437.000000|     1.625000|      62000.000000|\n",
            "|-114.610000|34.840000|         48.000000|1291.000000|    248.000000| 580.000000| 211.000000|     2.157100|      48600.000000|\n",
            "|-114.610000|34.830000|         31.000000|2478.000000|    464.000000|1346.000000| 479.000000|     3.212000|      70400.000000|\n",
            "|-114.630000|32.760000|         15.000000|1448.000000|    378.000000| 949.000000| 300.000000|     0.858500|      45000.000000|\n",
            "|-114.650000|34.890000|         17.000000|2556.000000|    587.000000|1005.000000| 401.000000|     1.699100|      69100.000000|\n",
            "|-114.650000|33.600000|         28.000000|1678.000000|    322.000000| 666.000000| 256.000000|     2.965300|      94900.000000|\n",
            "|-114.650000|32.790000|         21.000000|  44.000000|     33.000000|  64.000000|  27.000000|     0.857100|      25000.000000|\n",
            "|-114.660000|32.740000|         17.000000|1388.000000|    386.000000| 775.000000| 320.000000|     1.204900|      44000.000000|\n",
            "+-----------+---------+------------------+-----------+--------------+-----------+-----------+-------------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si queremos integrar la primera fila como los nombres de las columnas hay que agregar una opción a la hora de cargar los datos en el DataFrame."
      ],
      "metadata": {
        "id": "xoQGO2FxmtNK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#La opción option('header','true')\n",
        "df_spark_col  = spark_session.read.option('header', 'true').csv('sample_data/california_housing_train.csv')\n",
        "df_spark_col"
      ],
      "metadata": {
        "id": "37zvYB-onzJt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25ed4f2c-5b98-4edf-b48f-364c8fdfe138"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[longitude: string, latitude: string, housing_median_age: string, total_rooms: string, total_bedrooms: string, population: string, households: string, median_income: string, median_house_value: string]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_spark_col.show()"
      ],
      "metadata": {
        "id": "w52J4vd0oIT4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfaeeec9-b6d4-453d-92c5-41cc322e1670"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+---------+------------------+-----------+--------------+-----------+-----------+-------------+------------------+\n",
            "|  longitude| latitude|housing_median_age|total_rooms|total_bedrooms| population| households|median_income|median_house_value|\n",
            "+-----------+---------+------------------+-----------+--------------+-----------+-----------+-------------+------------------+\n",
            "|-114.310000|34.190000|         15.000000|5612.000000|   1283.000000|1015.000000| 472.000000|     1.493600|      66900.000000|\n",
            "|-114.470000|34.400000|         19.000000|7650.000000|   1901.000000|1129.000000| 463.000000|     1.820000|      80100.000000|\n",
            "|-114.560000|33.690000|         17.000000| 720.000000|    174.000000| 333.000000| 117.000000|     1.650900|      85700.000000|\n",
            "|-114.570000|33.640000|         14.000000|1501.000000|    337.000000| 515.000000| 226.000000|     3.191700|      73400.000000|\n",
            "|-114.570000|33.570000|         20.000000|1454.000000|    326.000000| 624.000000| 262.000000|     1.925000|      65500.000000|\n",
            "|-114.580000|33.630000|         29.000000|1387.000000|    236.000000| 671.000000| 239.000000|     3.343800|      74000.000000|\n",
            "|-114.580000|33.610000|         25.000000|2907.000000|    680.000000|1841.000000| 633.000000|     2.676800|      82400.000000|\n",
            "|-114.590000|34.830000|         41.000000| 812.000000|    168.000000| 375.000000| 158.000000|     1.708300|      48500.000000|\n",
            "|-114.590000|33.610000|         34.000000|4789.000000|   1175.000000|3134.000000|1056.000000|     2.178200|      58400.000000|\n",
            "|-114.600000|34.830000|         46.000000|1497.000000|    309.000000| 787.000000| 271.000000|     2.190800|      48100.000000|\n",
            "|-114.600000|33.620000|         16.000000|3741.000000|    801.000000|2434.000000| 824.000000|     2.679700|      86500.000000|\n",
            "|-114.600000|33.600000|         21.000000|1988.000000|    483.000000|1182.000000| 437.000000|     1.625000|      62000.000000|\n",
            "|-114.610000|34.840000|         48.000000|1291.000000|    248.000000| 580.000000| 211.000000|     2.157100|      48600.000000|\n",
            "|-114.610000|34.830000|         31.000000|2478.000000|    464.000000|1346.000000| 479.000000|     3.212000|      70400.000000|\n",
            "|-114.630000|32.760000|         15.000000|1448.000000|    378.000000| 949.000000| 300.000000|     0.858500|      45000.000000|\n",
            "|-114.650000|34.890000|         17.000000|2556.000000|    587.000000|1005.000000| 401.000000|     1.699100|      69100.000000|\n",
            "|-114.650000|33.600000|         28.000000|1678.000000|    322.000000| 666.000000| 256.000000|     2.965300|      94900.000000|\n",
            "|-114.650000|32.790000|         21.000000|  44.000000|     33.000000|  64.000000|  27.000000|     0.857100|      25000.000000|\n",
            "|-114.660000|32.740000|         17.000000|1388.000000|    386.000000| 775.000000| 320.000000|     1.204900|      44000.000000|\n",
            "|-114.670000|33.920000|         17.000000|  97.000000|     24.000000|  29.000000|  15.000000|     1.265600|      27500.000000|\n",
            "+-----------+---------+------------------+-----------+--------------+-----------+-----------+-------------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como comparativa entre Pandas y PySpark ambos manejan la información dentro de DataFrames pero la función *show()* solo es aplicable en Spark mientras que *head()* funciona en ambos"
      ],
      "metadata": {
        "id": "SkGw3U3UpiD7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(df_spark_col))\n",
        "print(type(data))"
      ],
      "metadata": {
        "id": "OBoQNiUVoSD4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4283ef3-b025-419b-ce28-8421e52f0ef9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pyspark.sql.dataframe.DataFrame'>\n",
            "<class 'pandas.core.frame.DataFrame'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La función *head()* muestra por cada columna el valor que tiene, sin embargo muestra la información por fila utilizando este formato mencionado"
      ],
      "metadata": {
        "id": "M4l2FDVMqRI3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_spark_col.head(10)"
      ],
      "metadata": {
        "id": "B2FAh9QDoha_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52c36f74-26bd-4107-959b-02b382d2ae65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(longitude='-114.310000', latitude='34.190000', housing_median_age='15.000000', total_rooms='5612.000000', total_bedrooms='1283.000000', population='1015.000000', households='472.000000', median_income='1.493600', median_house_value='66900.000000'),\n",
              " Row(longitude='-114.470000', latitude='34.400000', housing_median_age='19.000000', total_rooms='7650.000000', total_bedrooms='1901.000000', population='1129.000000', households='463.000000', median_income='1.820000', median_house_value='80100.000000'),\n",
              " Row(longitude='-114.560000', latitude='33.690000', housing_median_age='17.000000', total_rooms='720.000000', total_bedrooms='174.000000', population='333.000000', households='117.000000', median_income='1.650900', median_house_value='85700.000000'),\n",
              " Row(longitude='-114.570000', latitude='33.640000', housing_median_age='14.000000', total_rooms='1501.000000', total_bedrooms='337.000000', population='515.000000', households='226.000000', median_income='3.191700', median_house_value='73400.000000'),\n",
              " Row(longitude='-114.570000', latitude='33.570000', housing_median_age='20.000000', total_rooms='1454.000000', total_bedrooms='326.000000', population='624.000000', households='262.000000', median_income='1.925000', median_house_value='65500.000000'),\n",
              " Row(longitude='-114.580000', latitude='33.630000', housing_median_age='29.000000', total_rooms='1387.000000', total_bedrooms='236.000000', population='671.000000', households='239.000000', median_income='3.343800', median_house_value='74000.000000'),\n",
              " Row(longitude='-114.580000', latitude='33.610000', housing_median_age='25.000000', total_rooms='2907.000000', total_bedrooms='680.000000', population='1841.000000', households='633.000000', median_income='2.676800', median_house_value='82400.000000'),\n",
              " Row(longitude='-114.590000', latitude='34.830000', housing_median_age='41.000000', total_rooms='812.000000', total_bedrooms='168.000000', population='375.000000', households='158.000000', median_income='1.708300', median_house_value='48500.000000'),\n",
              " Row(longitude='-114.590000', latitude='33.610000', housing_median_age='34.000000', total_rooms='4789.000000', total_bedrooms='1175.000000', population='3134.000000', households='1056.000000', median_income='2.178200', median_house_value='58400.000000'),\n",
              " Row(longitude='-114.600000', latitude='34.830000', housing_median_age='46.000000', total_rooms='1497.000000', total_bedrooms='309.000000', population='787.000000', households='271.000000', median_income='2.190800', median_house_value='48100.000000')]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si queremos saber información acerca de los datos utilizamos la función *printSchema()* la cual muestra el nombre de cada columna, su tipo de dato y si permite valores nulos"
      ],
      "metadata": {
        "id": "4tNzS6dPrfns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_spark_col.printSchema()"
      ],
      "metadata": {
        "id": "_bdNEtSRokEQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ce6156b-7c7f-40fc-ee24-a0c6d1227d6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- longitude: string (nullable = true)\n",
            " |-- latitude: string (nullable = true)\n",
            " |-- housing_median_age: string (nullable = true)\n",
            " |-- total_rooms: string (nullable = true)\n",
            " |-- total_bedrooms: string (nullable = true)\n",
            " |-- population: string (nullable = true)\n",
            " |-- households: string (nullable = true)\n",
            " |-- median_income: string (nullable = true)\n",
            " |-- median_house_value: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tutorial 2: Consultas en DataFrame dentro de PySpark "
      ],
      "metadata": {
        "id": "h-buZtUBtnOp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('OperacionesFiltrado').getOrCreate()\n",
        "spark"
      ],
      "metadata": {
        "id": "tgvc4GJCqYPD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "eb5d1557-10a1-4824-ff50-97a9a44c49b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7ffaa1f26050>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://9058f9c207bc:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.2.2</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>PySpark_prueba1</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para cargar la información usaremos solamente la función *csv()* pero agregando parámetros de configuración para que tome el primer registro como los nombres de las columnas y tambien que a partir de los datos de entrada infiera el tipo de dato. Si no colocamos esto automáticamente considera de tipo *string* las columnas."
      ],
      "metadata": {
        "id": "Xa09MN6CwPEq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_filter_pyspark = spark.read.csv('part2.csv', header = True, inferSchema=True)\n",
        "df_filter_pyspark.show()"
      ],
      "metadata": {
        "id": "n87v4tRhqpZe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "outputId": "27beee03-c451-49c7-9a0e-c255d5ba484f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-70dd2a123502>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_filter_pyspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'part2.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_filter_pyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.2.2-bin-hadoop3.2/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.2.2-bin-hadoop3.2/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.2.2-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: Path does not exist: file:/content/part2.csv"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_filter_pyspark.printSchema()"
      ],
      "metadata": {
        "id": "4neGDBsyvX-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si se necesita podemos renombrar las columnas para referirnos a ellas de una forma más sencilla o simplificada con la función *withColumnRenamed()*."
      ],
      "metadata": {
        "id": "uWV3LXYuw0IQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filtros y selección"
      ],
      "metadata": {
        "id": "mr5GDyV93GOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_filter_pyspark= df_filter_pyspark.withColumnRenamed(\"Salary (per month - $)\",\"EmpSalary\")\n",
        "df_filter_pyspark.show()"
      ],
      "metadata": {
        "id": "_B5aqNbGrK_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "La función *filter()* nos permite filtrar la información a través de condiciones. Por ejemplo, vamos a mostrar unicamente aquellos empleados que tengan un salario menor o igual a $25,000.00."
      ],
      "metadata": {
        "id": "fUV8l1eWy90u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_filter_pyspark.filter(\"EmpSalary<=25000\").show()"
      ],
      "metadata": {
        "id": "FGGtrHVxq1qB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como pudieron observar hay una cierta similitud de la función *filter()* con SELECT de SQL. Es por eso que se pueden utilizar consultas SQL y tratar los DataFrames como tablas o vistas de un modelo relacional. La función *createOrReplaceTempView()* registra el DataFrame como una vista temporal dentro de la sesión que puede ejecutar consutlas SQL."
      ],
      "metadata": {
        "id": "gO6txvqVzdZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_filter_pyspark.createOrReplaceTempView(\"empleados\")\n",
        "sqlDF = spark.sql(\"SELECT * FROM empleados\")\n",
        "sqlDF.show()"
      ],
      "metadata": {
        "id": "CSkcljBNxiKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si la vista temporal que se produce quieren que sea utilizada en multiples sesiones entonces hay que utilizar la función *createOrReplaceTempView()*. El unico detalle es que la vista quedará anclada a una base de datos llamada *global_temp*."
      ],
      "metadata": {
        "id": "_4UHtcD20q8i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_filter_pyspark.createGlobalTempView(\"g_empleados\")\n",
        "sqlDF = spark.sql(\"SELECT * FROM global_temp.g_empleados\")\n",
        "sqlDF.show()"
      ],
      "metadata": {
        "id": "oNc4FMzG1Rln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como mencionamos la vista perdura en otras sesiones."
      ],
      "metadata": {
        "id": "6ATcN_3S1162"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.newSession().sql(\"SELECT * FROM global_temp.g_empleados\").show()"
      ],
      "metadata": {
        "id": "cR9qBGWh1-rz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se puede de igual forma cambiar el nombre de multiples columnas al mismo tiempo."
      ],
      "metadata": {
        "id": "lOw7X7R92H1a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Cambiamos el nombre de multiples columnas \n",
        "df_filter_pyspark= df_filter_pyspark.withColumnRenamed(\"Age of Employee\",\"EmpAge\").withColumnRenamed(\"Employee Name\",\"EmpName\")\n",
        "df_filter_pyspark.show()"
      ],
      "metadata": {
        "id": "_vkSgApfrlzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Al igual que en SQL se pueden seleccionar las columnas que serán mostradas dentro de la consulta acompañando a la función *filter()* con la función *select()*."
      ],
      "metadata": {
        "id": "73Rl2uG32Q_Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_filter_pyspark.filter(\"EmpSalary<=25000\").select(['EmpName','EmpAge']).show()"
      ],
      "metadata": {
        "id": "tGtFqXeyri3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Otra manera de filtrar la información de los registros es utilizar un estilo similar a Pandas."
      ],
      "metadata": {
        "id": "NXx1CwoD2sW1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_filter_pyspark.filter(df_filter_pyspark['EmpSalary']<=25000).select(['EmpName','EmpAge']).show()"
      ],
      "metadata": {
        "id": "LOZuURvRr4HZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Operadores Lógicos"
      ],
      "metadata": {
        "id": "FlPM5Da224mk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Los operadores lógicos disponibles son AND (&), OR (|) y NOT (~)."
      ],
      "metadata": {
        "id": "pHT4extP4NfO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejemplo con AND: Los empleados que su salario sea menor o igual a \\$30,000.00 y que sea mayor o igual a \\$18,000.00."
      ],
      "metadata": {
        "id": "gOTdAcq64fFd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_filter_pyspark.filter((df_filter_pyspark['EmpSalary']<=30000)\n",
        "                          & (df_filter_pyspark['EmpSalary']>=18000)).show()"
      ],
      "metadata": {
        "id": "vYgWyHBUsDw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cambiamos el nombre de la columna experiencia\n",
        "df_filter_pyspark= df_filter_pyspark.withColumnRenamed(\"Experience (in years)\",\"EmpExperience\")\n",
        "df_filter_pyspark.show()"
      ],
      "metadata": {
        "id": "FjuqaJ80syOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejemplo con OR: Los empleados que su salario sea menor o igual a \\$30,000.00 ó que su experiencia laboral sea mayor o igual a 3 años."
      ],
      "metadata": {
        "id": "latXscc144pR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_filter_pyspark.filter((df_filter_pyspark['EmpSalary']<=30000)\n",
        "                          | (df_filter_pyspark['EmpExperience']>=3)).show()"
      ],
      "metadata": {
        "id": "WxIIaEmvsvvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejemplo NOT: Los empleandos que su edad no sea mayor o igual a 30 años."
      ],
      "metadata": {
        "id": "WFxPQ4OL5YIR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_filter_pyspark.filter(~(df_filter_pyspark['EmpAge']>=30)).show()"
      ],
      "metadata": {
        "id": "ohEM94QPs9UE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tutorial 3: Manejo de valores nulos"
      ],
      "metadata": {
        "id": "ikvKjBuY6e7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creamos una sesión de PySpark\n",
        "from pyspark.sql import SparkSession\n",
        "#spark.stop()\n",
        "null_spark = SparkSession.builder.appName('ValoresNulos').getOrCreate()\n",
        "null_spark"
      ],
      "metadata": {
        "id": "2WFOahsM6utd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cargamos los datos dentro de un DataFrame de la sesión\n",
        "df_null_pyspark = null_spark.read.csv('part2.csv', header = True, inferSchema = True)\n",
        "df_null_pyspark"
      ],
      "metadata": {
        "id": "O5TJdLAp7ASc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualizamos la información\n",
        "df_null_pyspark.show()"
      ],
      "metadata": {
        "id": "8NXstlu57QSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Nuevamente vemos la estructura de la información\n",
        "# recordando que cuando tenemos nullable = true significa que esa columna permite \n",
        "# valores nulos\n",
        "df_null_pyspark.printSchema()"
      ],
      "metadata": {
        "id": "eAflsQi47elO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El función para eliminar los valores nulos dentro de la información es *na.drop()*. Esta función elimina completamente los registros que tiene algún valor nulo.\n",
        "\n"
      ],
      "metadata": {
        "id": "KPVY4QKT728z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_null_pyspark.na.drop().show()"
      ],
      "metadata": {
        "id": "Nk7qANkD72Z4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si queremos controlar el como se eliminan los registros la función tiene un parámetro llamado *how* con dos posibles valores:\n",
        "\n",
        "\n",
        "*   ALL: Elimina la tupla siempre y cuando todos los valores asociados a cada columna sean nulos.\n",
        "*   ANY: Elimina la tupla si alguno de los valores asociados a cada columns es nulo. Esta es la configuración por default.\n",
        "\n"
      ],
      "metadata": {
        "id": "HULRD2Mr8WUI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_null_pyspark.na.drop(how=\"all\").show()"
      ],
      "metadata": {
        "id": "KI5INUDi9DCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_null_pyspark.na.drop(how=\"any\").show()"
      ],
      "metadata": {
        "id": "M7EqNcvQ9G02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tambien hay forma de especificar el número mínimo de valores nulos aceptables con el parámetro *thresh*. En el ejemplo se puede observar que elimina solo una tupla que tenia tres valores nulos asociados."
      ],
      "metadata": {
        "id": "6-m7EXqo9L8f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_null_pyspark.na.drop(thresh=2).show()"
      ],
      "metadata": {
        "id": "QEZfXpS49XDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "De igual forma podemos combiar el parámetro *how* con *subset* para indicarle las columnas donde nos interesan detectar valores nulos en las tuplas y eliminarlas."
      ],
      "metadata": {
        "id": "HKMI4Qmc9yji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_null_pyspark.na.drop(how='any', subset=['Experience (in years)']).show()"
      ],
      "metadata": {
        "id": "lhma9LrN9ya0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos rellenar los valores nulos con algún valor en especifico utilizando la función *na.fill()* indicando el valor y la columna."
      ],
      "metadata": {
        "id": "SKkz--0i-Oyk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_null_pyspark.na.fill('NA values', 'Employee Name').show()"
      ],
      "metadata": {
        "id": "IAMqYGjo-krE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Otra alternativa para rellenar los valores faltantes es utilizando el método de imputación de datos utilizando la media. Para esto hay que utilizar la clase *Imputer* especificando las columnas de entrada y las de salida que se agregaran al DataFrame así como la estrategia en este caso utilizar la media. Después, se utiliza la función *fit()* y *transform()* para integrar las columnas imputadas."
      ],
      "metadata": {
        "id": "GbXa3gu6-u4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import Imputer\n",
        "\n",
        "imputer = Imputer(\n",
        "    inputCols = ['Age of Employee', 'Experience (in years)', 'Salary (per month - $)'],\n",
        "    outputCols = [\"{}_imputed\".format(a) for a in ['Age of Employee', 'Experience (in years)', 'Salary (per month - $)']]\n",
        ").setStrategy(\"mean\")\n",
        "imputer.fit(df_null_pyspark).transform(df_null_pyspark).show()"
      ],
      "metadata": {
        "id": "8FAlh9Sy-vBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tutorial 4: Manejo de DataFrames en PySpark"
      ],
      "metadata": {
        "id": "7_zMjiAbClef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creamos la sesión de trabajo\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "data_spark = SparkSession.builder.appName('DataFrame_article').getOrCreate()\n",
        "data_spark"
      ],
      "metadata": {
        "id": "gWBdsz42Czua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cargamos los datos e imprimimos la descripción del schema\n",
        "df_pyspark = data_spark.read.option('header','true').csv('/content/sample_data/california_housing_train.csv', inferSchema=True)\n",
        "df_pyspark.printSchema()"
      ],
      "metadata": {
        "id": "4I1c0TjrC9Sv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualizamos la información\n",
        "df_pyspark.show()"
      ],
      "metadata": {
        "id": "d5q5LqAcMVM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En caso de querer cambiar el tipo de dato de alguna columna lo podemos hacer con las funciones *withColumn()* y *cast()*."
      ],
      "metadata": {
        "id": "WSz7YI4wMscN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import column\n",
        "df_pyspark=df_pyspark.withColumn(\"housing_median_age\",column(\"housing_median_age\").cast(\"int\"))"
      ],
      "metadata": {
        "id": "XWt7j0x4MslB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con el atributo *dtypes* podemos saber el tipo de dato por columna"
      ],
      "metadata": {
        "id": "_Oubrma8MEU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_pyspark.dtypes"
      ],
      "metadata": {
        "id": "5QJEKDVHDHrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si queremos saber el nombre de las columnas utilizamos el atributo *columns*"
      ],
      "metadata": {
        "id": "ghry7pbYN43p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_pyspark.columns"
      ],
      "metadata": {
        "id": "-RjGvsq_DT4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "También, se puede seleccionar todos los datos de una columna en particular con la función *select()*."
      ],
      "metadata": {
        "id": "YdcYuuXAOBr3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_pyspark.select('total_rooms').show()"
      ],
      "metadata": {
        "id": "9k3Qp6F9DWVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O en caso de querer seleccionar varias columnas tambien se puede lograr enviando una lista con el nombre de las columnas como parámetro."
      ],
      "metadata": {
        "id": "q5T8sf9yOUxs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_pyspark.select(['total_rooms', 'total_bedrooms', 'median_income']).show()"
      ],
      "metadata": {
        "id": "_JpH1i60DZVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si queremos saber algunas medidas de tendencia central de los datos para los análisis estadísticos se puede utilizar la función *describe()* similar a Pandas."
      ],
      "metadata": {
        "id": "y3KgSo5MOg6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_pyspark.describe().show()"
      ],
      "metadata": {
        "id": "xfKnLrhzDbua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "De igual manera se pueden agregar columnas directamente al DataFrame si se requiere."
      ],
      "metadata": {
        "id": "eE5gL9snPdsE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_pyspark = df_pyspark.withColumn('Updated_medianhousevalue', df_pyspark['median_house_value']*2)\n",
        "df_pyspark.show()"
      ],
      "metadata": {
        "id": "t38O8t0cDfY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "De igual forma se pueden eliminar con la función *drop()*"
      ],
      "metadata": {
        "id": "VHB46MbDPnmZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_pyspark.drop('Updated_medianhousevalue').show()"
      ],
      "metadata": {
        "id": "F5Vp00etD_LX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tutorial 5: Agregación y agrupamientos\n",
        "\n",
        "Agrupar los datos es una de las habilidades más esenciales cuando trabajamos con Big Data dado que estamos tratando con una gran cantidad de datos y si no somos capaces de segmentar esos datos, entonces será mucho más difícil analizarlos y usarlos para obtener información relevante\n",
        "\n",
        "La regla de oro es recordar que la función *groupBy()* y la función de agregación van de la mano, es decir, no podemos usar groupBy sin la función agregada como SUM, COUNT, AVG, MAX, MIN, etc."
      ],
      "metadata": {
        "id": "cYc2mWGlPxFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark_aggregate = SparkSession.builder.appName('Aggregate and GroupBy').getOrCreate()\n",
        "spark_aggregate"
      ],
      "metadata": {
        "id": "pCMVYeyoT0dq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark_aggregate_data = spark_aggregate.read.csv('part4.csv', header = True, inferSchema = True)\n",
        "spark_aggregate_data.show()"
      ],
      "metadata": {
        "id": "L8YpcNuaTtUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si llegamos a ejecutar unicamente la función *groupBy()* la respuesta será la ubicación de los datos agrupados lo cual no es relevante"
      ],
      "metadata": {
        "id": "j74wnJlET8Gk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark_aggregate_data.groupBy('Name')"
      ],
      "metadata": {
        "id": "R9mnirSyS6O5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Funciones de agregación\n",
        "\n",
        "Algunas de las funciones más comunes son:\n",
        "\n",
        "\n",
        "\n",
        "*   AVG: devuelve el conjunto de resultados agrupando la columna según el promedio del conjunto de valores.\n",
        "*   COUNT: devolverá el número total de conjuntos de valores en una columna particular correspondiente a la función groupBy.\n",
        "*   MIN: devuelve el valor mínimo o más pequeño entre todo el conjunto de valores en toda la fila.\n",
        "*   MAX: el funcionamiento y el enfoque de usar la función agregada MAX es el mismo que la función agregada MIN, solo que la principal diferencia es que devolverá el valor máximo entre el conjunto de valores en la fila.\n",
        "*   SUM: devolverá la suma de todos los valores numéricos correspondientes a la columna agrupada\n",
        "\n"
      ],
      "metadata": {
        "id": "AAlB-sLiUZGS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si ejecutamos la función de agrupamiento y agregación el resultado será la descripción del DataFrame por lo que si queremos visualizar la información hay que utilizar la función *show()*."
      ],
      "metadata": {
        "id": "KGaco-YtWYNH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark_aggregate_data.groupBy('Name').sum()"
      ],
      "metadata": {
        "id": "y-35ZH9tVBHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejemplo: Conocer la cantidad de dinero total que le pago la compañia a cada empleado agrupando por nombre"
      ],
      "metadata": {
        "id": "q54ThS92XAqX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark_aggregate_data.groupBy('Name').sum().show()"
      ],
      "metadata": {
        "id": "AHDQqxfGW0PD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejemplo: Conocer la cantidad de dinero total que pago cada departamento a sus empleados"
      ],
      "metadata": {
        "id": "qyyW7WWZXHx4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark_aggregate_data.groupBy('Departmens').sum().show()"
      ],
      "metadata": {
        "id": "c5st2YW3XPIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejemplo: Conocer el salario promedio que se le pago a los empleados por departamento"
      ],
      "metadata": {
        "id": "3imE2OXvXaIx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark_aggregate_data.groupBy('Departmens').mean().show()"
      ],
      "metadata": {
        "id": "EdtCu_hsXnO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejemplo: Saber el número de pagos que recibio cada empleado"
      ],
      "metadata": {
        "id": "MlhwGhpkXylB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark_aggregate_data.groupBy(['Name']).count().show()"
      ],
      "metadata": {
        "id": "oEowM03EXywJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tutorial 6: Usando ML en PySpark"
      ],
      "metadata": {
        "id": "MHhiL2rneKF5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "df_ml = SparkSession.builder.appName('EjemploML').getOrCreate()\n",
        "df_ml"
      ],
      "metadata": {
        "id": "lzkltL3aeXm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cargamos la información en un DataFrame\n",
        "training_dataset  = df_ml.read.csv('UserCarDataExample.csv', header=True, inferSchema=True)\n",
        "training_dataset"
      ],
      "metadata": {
        "id": "tumrUfnSf8z4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Mostramos la información\n",
        "training_dataset.show()"
      ],
      "metadata": {
        "id": "Kb1J2b66gLEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualizamos el esquema de la base de datos\n",
        "training_dataset.printSchema()"
      ],
      "metadata": {
        "id": "0H4E0wYKgOnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualizamos el nombre de las columnas\n",
        "training_dataset.columns"
      ],
      "metadata": {
        "id": "QQkaCFvAgRL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para trabajar con modelos de regresión tenemos que utilizar *VectorAssembler* para convertir las variables independientes en un vector que las incluya"
      ],
      "metadata": {
        "id": "moU5VyLK8tlJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "featassembler = VectorAssembler(inputCols=['age',\n",
        " 'km_driven',\n",
        " 'mileage',\n",
        " 'engine',\n",
        " 'max_power',\n",
        " 'seats'], outputCol = \"Independent Features\" )\n",
        "featassembler"
      ],
      "metadata": {
        "id": "bw-DruM-gkRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Posteriormente se integran al conjunto de datos que ya estaba cargado utilizando la función *transform()*."
      ],
      "metadata": {
        "id": "-j2BdYSb9LCY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = featassembler.transform(training_dataset)\n",
        "result.show()"
      ],
      "metadata": {
        "id": "aSF0ijSFgsqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para construir nuestro modelo de regresión debemos selecccionar la columna que integró los valores de las columnas en un vector y la columna que representa la variable dependiente."
      ],
      "metadata": {
        "id": "rEIplNaj9aTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_data = result.select(\"Independent features\", \"selling_price\")\n",
        "final_data.show()"
      ],
      "metadata": {
        "id": "AkN24b3_gxBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Del conjunto total de datos se puede generar un división de conjunto de datos entre entrenamiento y prueba con la función *randomSplit*."
      ],
      "metadata": {
        "id": "PXw3mdLr-ieZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data = final_data.randomSplit([0.75, 0.25])"
      ],
      "metadata": {
        "id": "iRqfuLCSg3WF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora bien hay que importar *LinearRegression* de la biblioteca de machine learning de PySpark. Especificando cuales son la variables independientes y cual es la dependiente."
      ],
      "metadata": {
        "id": "1enV5l9G-ul7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.regression import LinearRegression\n",
        "\n",
        "model = LinearRegression(featuresCol = 'Independent features', labelCol='selling_price')\n",
        "model = model.fit(train_data)"
      ],
      "metadata": {
        "id": "XzyfNi_5g6Ro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos imprimir la matriz de correlación para verificar la congruencia del modelo."
      ],
      "metadata": {
        "id": "hWZ7qUpPANhH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.stat import Correlation\n",
        "\n",
        "matrix = Correlation.corr(final_data, 'Independent features')\n",
        "cor_np = matrix.collect()[0][matrix.columns[0]].toArray()\n",
        "cor_np"
      ],
      "metadata": {
        "id": "q_PZNczQjDUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obtener el valor del intercepto."
      ],
      "metadata": {
        "id": "K0eiZUdrAUKW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.intercept"
      ],
      "metadata": {
        "id": "hRPAwuMDhGsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Los coeficientes por cada variable independiente"
      ],
      "metadata": {
        "id": "LWm_ACuzAXhS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.coefficients"
      ],
      "metadata": {
        "id": "RmOSQXazhC4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Así como los p-values para determinar la transendencia de cada variable dentro del modelo."
      ],
      "metadata": {
        "id": "SvVYUVYQAbst"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary.pValues"
      ],
      "metadata": {
        "id": "DvpGrjq1iEvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obtener indicadores de desempeño como la $r^2$ ajustada, dado que es un problema multivariado. Que nos sirve para indicar el porcentaje de la variabilidad de la variable dependiente explicada por el modelo."
      ],
      "metadata": {
        "id": "bz0Mb-N_AkKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary.r2adj"
      ],
      "metadata": {
        "id": "Pi-wb7QUifex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos realizar predicciones para evaluar el modelo obtenido."
      ],
      "metadata": {
        "id": "g5rpjApHA12V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_result = model.evaluate(test_data)\n",
        "prediction_result.predictions.show()"
      ],
      "metadata": {
        "id": "gHIihH1RhJ-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mostrar algunos indicadores de desempeño utiles."
      ],
      "metadata": {
        "id": "DzBuAmmiA7pk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_result.meanAbsoluteError, prediction_result.meanSquaredError"
      ],
      "metadata": {
        "id": "zdRto3yjhe_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tutorial 7: Utilizando MLlib en PySpark\n",
        "\n",
        "MLlib es la biblioteca de aprendizaje automático (ML) de Spark. Su objetivo es hacer que el aprendizaje automático práctico sea escalable y fácil a un alto nivel. La biblioteca proporciona herramientas como:\n",
        "\n",
        "Algoritmos de ML: algoritmos de aprendizaje comunes como clasificación, regresión, agrupamiento y filtrado colaborativo.\n",
        "Características: extracción de características, transformación, reducción de dimensionalidad y selección\n",
        "Pipelines: herramientas para construir, evaluar y ajustar ML Pipelines\n",
        "Persistencia: guardar y cargar algoritmos, modelos y Pipelines\n",
        "Utilidades: álgebra lineal, estadística, manejo de datos, etc."
      ],
      "metadata": {
        "id": "wJ4qq4PRBWK3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.mllib.tree import DecisionTree\n",
        "from pyspark.mllib.tree import DecisionTreeModel\n",
        "\n",
        "from pyspark import SparkContext\n",
        "\n",
        "sc = SparkContext.getOrCreate()"
      ],
      "metadata": {
        "id": "qmz6gEjCCGLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.mllib.util import MLUtils\n",
        "\n",
        "data = MLUtils.loadLibSVMFile(sc, 'spark-3.2.2-bin-hadoop3.2/data/mllib/sample_libsvm_data.txt')\n"
      ],
      "metadata": {
        "id": "eINQssUpC729"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and test sets (30% held out for testing)\n",
        "(trainingData, testData) = data.randomSplit([0.7, 0.3])"
      ],
      "metadata": {
        "id": "K_bVJzJ4YDZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.take(10)"
      ],
      "metadata": {
        "id": "Fo4frQjCFH26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numClasses = 2\n",
        "categoricalFeaturesInfo = {}\n",
        "impurity = \"gini\"\n",
        "\n",
        "model = DecisionTree.trainClassifier(trainingData, numClasses, categoricalFeaturesInfo,\n",
        "  impurity)"
      ],
      "metadata": {
        "id": "vEw7hMHsatv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(testData.map(lambda x: x.features))\n",
        "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
        "testErr = labelsAndPredictions.filter(\n",
        "    lambda lp: lp[0] != lp[1]).count() / float(testData.count())\n",
        "print('Test Error = ' + str(testErr))\n",
        "print('Learned classification tree model:')\n",
        "print(model.toDebugString())"
      ],
      "metadata": {
        "id": "XmTnplvuETu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(sc, \"myDecisionTreeClassificationModel.dt\")\n",
        "sameModel = DecisionTreeModel.load(sc, \"myDecisionTreeClassificationModel.dt\")"
      ],
      "metadata": {
        "id": "od6iDkJhIVm8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}